{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZA/uE707XJsigEfoBBHeN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arjunmalik11/AI-Agents-with-Autogen/blob/main/Autogen_Customer_Onboarding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_QEHmPpnnFD1"
      },
      "outputs": [],
      "source": [
        "OPENAI_API_KEY = \"secret-key\"\n",
        "llm_config = {\n",
        "    \"model\" : \"gpt-4\",\n",
        "    \"api_key\" : OPENAI_API_KEY,\n",
        " }"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogen"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8shukfJno9z",
        "outputId": "033094cc-711d-48b2-891b-6bddd2f3d9ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogen\n",
            "  Downloading autogen-0.3.0-py3-none-any.whl.metadata (26 kB)\n",
            "Collecting diskcache (from autogen)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting docker (from autogen)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting flaml (from autogen)\n",
            "  Downloading FLAML-2.2.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy<2,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from autogen) (1.26.4)\n",
            "Collecting openai>=1.3 (from autogen)\n",
            "  Downloading openai-1.44.1-py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from autogen) (24.1)\n",
            "Requirement already satisfied: pydantic!=2.6.0,<3,>=1.10 in /usr/local/lib/python3.10/dist-packages (from autogen) (2.8.2)\n",
            "Collecting python-dotenv (from autogen)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from autogen) (2.4.0)\n",
            "Collecting tiktoken (from autogen)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.3->autogen) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.3->autogen)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting jiter<1,>=0.4.0 (from openai>=1.3->autogen)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>=1.3->autogen) (4.12.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=2.6.0,<3,>=1.10->autogen) (2.20.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen) (2.32.3)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.10/dist-packages (from docker->autogen) (2.0.7)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->autogen) (2024.5.15)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen) (3.8)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.3->autogen) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.3->autogen) (2024.8.30)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.3->autogen)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.3->autogen)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->docker->autogen) (3.3.2)\n",
            "Downloading autogen-0.3.0-py3-none-any.whl (345 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m345.2/345.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.44.1-py3-none-any.whl (373 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m373.5/373.5 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading FLAML-2.2.0-py3-none-any.whl (297 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.2/297.2 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-dotenv, jiter, h11, flaml, diskcache, tiktoken, httpcore, docker, httpx, openai, autogen\n",
            "Successfully installed autogen-0.3.0 diskcache-5.6.3 docker-7.1.0 flaml-2.2.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jiter-0.5.0 openai-1.44.1 python-dotenv-1.0.1 tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import ConversableAgent\n",
        "\n",
        "onboarding_personal_information_agent = ConversableAgent(\n",
        "    name=\"Onboarding_Personal_Information_Agent\",\n",
        "    system_message='''You are a helpful customer onboarding agent,\n",
        "    you are here to help new customers get started with our product.\n",
        "    Your job is to gather customer's name and location.\n",
        "    Do not ask for other information. Return 'TERMINATE'\n",
        "    when you have gathered all the information.''',\n",
        "    llm_config=llm_config,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "onboarding_topic_preference_agent = ConversableAgent(\n",
        "    name=\"Onboarding_Topic_preference_Agent\",\n",
        "    system_message='''You are a helpful customer onboarding agent,\n",
        "    you are here to help new customers get started with our product.\n",
        "    Your job is to gather customer's preferences on news topics.\n",
        "    Do not ask for other information.\n",
        "    Return 'TERMINATE' when you have gathered all the information.''',\n",
        "    llm_config=llm_config,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        ")\n",
        "\n",
        "customer_engagement_agent = ConversableAgent(\n",
        "    name=\"Customer_Engagement_Agent\",\n",
        "    system_message='''You are a helpful customer service agent\n",
        "    here to provide fun for the customer based on the user's\n",
        "    personal information and topic preferences.\n",
        "    This could include fun facts, jokes, or interesting stories.\n",
        "    Make sure to make it engaging and fun!\n",
        "    Return 'TERMINATE' when you are done.''',\n",
        "    llm_config=llm_config,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"NEVER\",\n",
        "    is_termination_msg=lambda msg: \"terminate\" in msg.get(\"content\").lower(),\n",
        ")\n",
        "\n",
        "customer_proxy_agent = ConversableAgent(\n",
        "    name=\"customer_proxy_agent\",\n",
        "    llm_config=False,\n",
        "    code_execution_config=False,\n",
        "    human_input_mode=\"ALWAYS\",\n",
        "    is_termination_msg=lambda msg: \"terminate\" in msg.get(\"content\").lower(),\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jXspm1exnbOX",
        "outputId": "34a3a631-67d2-4829-a316-9ec15ee9c164"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 09-11 15:31:42] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 09-11 15:31:42] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[autogen.oai.client: 09-11 15:31:42] {184} WARNING - The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autogen.oai.client:The API key specified is not a valid OpenAI format; it won't work with the OpenAI-hosted model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from autogen import initiate_chats\n",
        "\n",
        "chats = [\n",
        "    {\n",
        "        \"sender\": onboarding_personal_information_agent,\n",
        "        \"recipient\": customer_proxy_agent,\n",
        "        \"message\": (\n",
        "            \"Hello, I'm here to help you get started with our product. \"\n",
        "            \"Could you tell me your name and location?\"\n",
        "        ),\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "        \"summary_args\": {\n",
        "            \"summary_prompt\": \"Return the customer information as a JSON object only: \"\n",
        "                              \"{\\\"name\\\": \\\"\\\", \\\"location\\\": \\\"\\\"}\",\n",
        "        },\n",
        "        \"max_turns\": 2,\n",
        "        \"clear_history\": True\n",
        "    },\n",
        "    {\n",
        "        \"sender\": onboarding_topic_preference_agent,\n",
        "        \"recipient\": customer_proxy_agent,\n",
        "        \"message\": (\n",
        "            \"Great! Could you tell me what topics you are \"\n",
        "            \"interested in reading about?\"\n",
        "        ),\n",
        "        \"summary_method\": \"reflection_with_llm\",\n",
        "        \"max_turns\": 1,\n",
        "        \"clear_history\": False\n",
        "    },\n",
        "    {\n",
        "        \"sender\": customer_proxy_agent,\n",
        "        \"recipient\": customer_engagement_agent,\n",
        "        \"message\": \"Let's find something fun to read.\",\n",
        "        \"max_turns\": 1,\n",
        "        \"summary_method\": \"reflection_with_llm\"\n",
        "    }\n",
        "]\n",
        "\n",
        "chat_results = initiate_chats(chats)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YEJthZip5z-",
        "outputId": "45ca0a8c-ca5e-4476-b60e-530ab91e3db6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Onboarding_Personal_Information_Agent (to customer_proxy_agent):\n",
            "\n",
            "Hello, I'm here to help you get started with our product. Could you tell me your name and location?\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as customer_proxy_agent. Provide feedback to Onboarding_Personal_Information_Agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: Frank and Austin\n",
            "customer_proxy_agent (to Onboarding_Personal_Information_Agent):\n",
            "\n",
            "Frank and Austin\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Onboarding_Personal_Information_Agent (to customer_proxy_agent):\n",
            "\n",
            "Nice to meet you, Frank from Austin. Welcome to our community! TERMINATE\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as customer_proxy_agent. Provide feedback to Onboarding_Personal_Information_Agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: thank you\n",
            "customer_proxy_agent (to Onboarding_Personal_Information_Agent):\n",
            "\n",
            "thank you\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "Onboarding_Topic_preference_Agent (to customer_proxy_agent):\n",
            "\n",
            "Great! Could you tell me what topics you are interested in reading about?\n",
            "Context: \n",
            "{\"name\": \"Frank\", \"location\": \"Austin\"}\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Replying as customer_proxy_agent. Provide feedback to Onboarding_Topic_preference_Agent. Press enter to skip and use auto-reply, or type 'exit' to end the conversation: music\n",
            "customer_proxy_agent (to Onboarding_Topic_preference_Agent):\n",
            "\n",
            "music\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "********************************************************************************\n",
            "Starting a new chat....\n",
            "\n",
            "********************************************************************************\n",
            "customer_proxy_agent (to Customer_Engagement_Agent):\n",
            "\n",
            "Let's find something fun to read.\n",
            "Context: \n",
            "{\"name\": \"Frank\", \"location\": \"Austin\"}\n",
            "The user, Frank, is interested in reading about the topic of music.\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Customer_Engagement_Agent (to customer_proxy_agent):\n",
            "\n",
            "Hello Frank, nice to meet you! Ah, a music enthusiast in Austin – what a fantastic combination. Austin is famous for its burgeoning music scene after all!\n",
            "\n",
            "To kick us off, here's a fun little fact about music: Did you know Johann Sebastian Bach was more than just a notable music composer? It turns out, in addition to his considerable musical skills, he also conducted significant repairs on his own harpsichords and organs.\n",
            "\n",
            "Now for a little hometown pride... Did you know that Austin, Texas, holds the title of Live Music Capital of the World? The city boasts more live music venues per capita than anywhere else!\n",
            "\n",
            "And, to lighten things up a bit, here's a music-themed joke for you:\n",
            "\n",
            "Why did the pianist keep his piano keys on top of the wardrobe?\n",
            "\n",
            "Because he wanted to play high notes!\n",
            "\n",
            "I hope this brings a smile to your face, Frank. If you have any specific music genres or musicians you are interested in, feel free to share. I can dig up more facts or even some jokes tailored to your taste.\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chat_result in chat_results:\n",
        "    print(chat_result.summary)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "id": "mfWQO5hdqgaG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ed8b5322-e9d6-4cef-b871-e5bc4302b0b2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"name\": \"Frank\", \"location\": \"Austin\"}\n",
            "\n",
            "\n",
            "The user, Frank, is interested in reading about the topic of music.\n",
            "\n",
            "\n",
            "The user, Frank, is interested in music and might appreciate reading fun facts or humor related to this topic. Since he is located in Austin, Texas, known as the Live Music Capital of the World, music-related content connected to his location may also be of interest.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for chat_result in chat_results:\n",
        "    print(chat_result.cost)\n",
        "    print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCIjzar1t4mF",
        "outputId": "ba8cc343-3292-421e-b1d7-b73568f6035c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'usage_including_cached_inference': {'total_cost': 0.00849, 'gpt-4-0613': {'cost': 0.00849, 'prompt_tokens': 227, 'completion_tokens': 28, 'total_tokens': 255}}, 'usage_excluding_cached_inference': {'total_cost': 0.00849, 'gpt-4-0613': {'cost': 0.00849, 'prompt_tokens': 227, 'completion_tokens': 28, 'total_tokens': 255}}}\n",
            "\n",
            "\n",
            "{'usage_including_cached_inference': {'total_cost': 0.0031199999999999995, 'gpt-4-0613': {'cost': 0.0031199999999999995, 'prompt_tokens': 74, 'completion_tokens': 15, 'total_tokens': 89}}, 'usage_excluding_cached_inference': {'total_cost': 0.0031199999999999995, 'gpt-4-0613': {'cost': 0.0031199999999999995, 'prompt_tokens': 74, 'completion_tokens': 15, 'total_tokens': 89}}}\n",
            "\n",
            "\n",
            "{'usage_including_cached_inference': {'total_cost': 0.02841, 'gpt-4-0613': {'cost': 0.02841, 'prompt_tokens': 411, 'completion_tokens': 268, 'total_tokens': 679}}, 'usage_excluding_cached_inference': {'total_cost': 0.02841, 'gpt-4-0613': {'cost': 0.02841, 'prompt_tokens': 411, 'completion_tokens': 268, 'total_tokens': 679}}}\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XYxKUxTzuAiY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}